<!DOCTYPE html>
<style>
    html {
        height: 100vh;
    }
    body {
        min-height: 100vh;
        display: grid;
        grid-template-rows: 1fr auto;
    }
    .footer {
        grid-row-start: 2;
        grid-row-end: 3;
    }
</style>
<html lang="en-gb"><head>
  <meta charset="utf-8">
  <title>ZJUTVIS</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="description" property="og:description" content="The website of ZJUTVIS, an academic group, research in VIS, college of computer science, zhejiang university of technology">
  <meta name="keywords" content="">
  <meta name="image" property="og:image" content="/">
  <meta name="author" content="ZJUT VIS">
  <meta name="generator" content="Hugo 0.85.0" />

  <!-- plugins -->
  
  <link rel="stylesheet" href="https://zjutvis-1253917436.cos.ap-shanghai.myqcloud.com/cdn/uikit/uikit.min.css ">
  

  <!-- Main Stylesheet -->
  
  <link rel="stylesheet" href="/css/style.min.css" integrity="" media="screen">
  
  <link rel="stylesheet" href="/css/syntax.min.css" integrity="" media="screen">
  
  <link rel="stylesheet" href="/css/element_ui/index.min.css" integrity="" media="screen">

  
<script src="https://zjutvis-1253917436.cos.ap-shanghai.myqcloud.com/cdn/mailgo/mailgo.min.js" defer></script>

  <script src="https://zjutvis-1253917436.cos.ap-shanghai.myqcloud.com/cdn/element-ui/index.js"></script>

  <!--Favicon-->
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="shortcut icon" href="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com/images/biglogo.svg" type="image/x-icon">


</head>
<body class="research">
    <div class="wrapper"><style>
    @media screen and (max-width: 400px){
        .title-div{
            width: 30%;
            float: left;
            height: auto;
            text-align: center;
            margin-left: 1%;
        }
        .big-title {
            
            font-size: 1.2rem;
            font-weight: 900;
            color: rgba(25,30,41,0.9);
            text-shadow: 3px 3px 2px #aab5e6;
            margin-top: 0px;
            margin-bottom: 0px;
        }
        .small-title{
            position: relative;
            padding-left: 0.5rem;
            width: 25%;
            align-self: center;
            color: #474646;
        }
    }
    @media screen and (min-width: 400px){
        .title-div{
            width: auto;
            height: auto;
            float: left;
            text-align: center;
            margin-left: 1%;
        }
        .big-title {
            
            font-size: 1.7rem;
            font-weight: 900;
            color: rgba(25,30,41,0.9);
            text-shadow: 3px 3px 2px #aab5e6;
            margin-top: 0px;
            margin-bottom: 0px;
        }
        .small-title{
            position: relative;
            padding-left: 1rem;
            width: auto;
            align-self: center;
            color: #474646;
        }
    }
</style>

<div uk-sticky="sel-target: .uk-navbar-container; cls-active: uk-navbar-sticky; bottom: #transparent-sticky-navbar">
    <nav class="uk-navbar-container uk-margin" uk-navbar="mode: click;" style="padding-right: 3rem">

        <div class="title-div">
            <h1 class="big-title">
                <a style="text-decoration:none; color:rgba(25,30,41,0.9)" href=http://www.zjutvis.org/>
                    ZJUT VIS
                </a>
            </h1>
            <h2 class="big-title">
                <a style="text-decoration:none; color:rgba(25,30,41,0.9)" href=http://www.zjutvis.org/>
                    浙工大可视化小组
                </a>
            </h2>
        </div>
        <div class="small-title">
            <div>网站建设中</div>
        </div>
        <div class="uk-navbar-right">
            <ul class="uk-navbar-nav uk-visible@s">
                
                <li><a style="font-size: 18px; text-transform: none" href="/"> 首页
                        </a></li>

                
                
                
                
                
                
                
                
                
                <li class="uk-active"><a style="font-size: 18px; text-transform:none"
                        href="/research">科研成果</a></li>
                
                
                
                
                
                
                
                
                
                <li><a style="font-size: 18px; text-transform: none" href="/publication">科研论文</a></li>
                
                
                
                
                
                
                
                
                
                <li><a style="font-size: 18px; text-transform: none" href="/members">成员</a></li>
                
                
                
                
                
                
                
                
                
                <li><a style="font-size: 18px; text-transform: none" href="/about">简介</a></li>
                
                
            </ul>

            <a class="uk-navbar-toggle uk-hidden@s" uk-toggle="target: #sidenav" uk-navbar-toggle-icon href="#"></a>

        </div>
    </nav>
</div>

<div id="sidenav" uk-offcanvas="overlay: true">
    <div class="uk-offcanvas-bar uk-flex uk-flex-column">

        <ul class="uk-nav uk-nav-primary uk-nav-center uk-margin-auto-vertical">
            
            <li><a style="font-size: 18px" href="http://www.zjutvis.org/"> 首页 </a></li>

            
            
            
            
            
            
            
            
            
            <li class="uk-active"><a style="font-size: 18px" href="/research">科研成果</a></li>
            
            
            
            
            
            
            
            
            
            <li><a style="font-size: 18px" href="/publication">科研论文</a></li>
            
            
            
            
            
            
            
            
            
            <li><a style="font-size: 18px" href="/members">成员</a></li>
            
            
            
            
            
            
            
            
            
            <li><a style="font-size: 18px" href="/about">简介</a></li>
            
            
        </ul>

    </div>
</div>




<style>
  p {
    margin: 0
  }

  .people-wrap {
    display: inline-flex;
    margin-top: 10px;
  }

  .people-image {
    display: block;
    border-radius: 50%;
     
  }

  .people {
    margin-left: 20px;
    margin-right: 20px;
  }

  .name {
    margin: auto;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    color: #000;
    font-family: Roboto, sans-serif;
    line-height: 1.2;
    margin-top: 2rem;
  }

  .content img {
    box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);
    display: block;
    margin-left: auto;
    margin-right: auto;
    margin-bottom:30px;
  }

  p { margin-bottom: 1em; }

  body p:last-child { margin-bottom: 0em; }

  img.skills{
    height: 80px;
  }

  @media only screen and (min-width: 960px) {
    .content img{max-width:60vw; max-height:60vh}

  .content img{
      max-width:60vw;
      max-height:60vh
  }
  }
  .content{
      width: 90vw;
      max-width: 1200px
  }

  table {
    font-family: arial, sans-serif;
    border-collapse: collapse;
    width: 100%;
  }

  td, th {
    border: 1px solid #dddddd;
    text-align: left;
    padding: 8px;
  }











</style>

<section>
    

    

    
    
    <div class="uk-container uk-margin-large-bottom uk-margin-medium-top">
        <h1 class="uk-text-center uk-heading-medium">视频数据可视分析</h1>
        <p class="uk-text-center uk-text-lead paper-abstract" style="text-align-last: left;">在国家自然科学基金重点项目等资助下，围绕跨设备多场景视频的快速理解展开研究。具体探索如何将“人在回路”强关联至视频语义模型的理解，充分利用可视分析技术并在内容理解中融合用户意图，提出了多粒度精细化可视分析、面向多层级多特征的可解释可视分析等系列方法，相关成果发表于IEEE TVCG、TMM、TBD、ACM TIST等，获得中国可视化与可视分析大会最佳论文提名奖等。</p>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <ul
                class="js-filter uk-child-width-1-1 uk-child-width-1-2@m uk-text-center"
                uk-grid
                uk-height-match="target: > li > div > .fixy"
        >
            
            
            
            
            
            
            
            <li data-tags="Visualization has the capacity of converting auditory perceptions of music into visual perceptions, which consequently opens the door to music visualization (e.g., exploring group style transitions and analyzing performance details). Current research either focuses on low-level analysis without constructing and comparing music group characteristics, or concentrates on high-level group analysis without analyzing and exploring detailed information. To fill this gap, integrating the high-level group analysis and low-level details exploration of music, we design a musical semantic sequence visualization analytics prototype system (MUSE) that mainly combines a distribution view and a semantic detail view, assisting analysts in obtaining the group characteristics and detailed interpretation. In the MUSE, we decompose the music into note sequences for modeling and abstracting music into three progressively fine-grained pieces of information (i.e., genres, instruments and notes). The distribution view integrates a new density contour, which considers sequence distance and semantic similarity, and helps analysts quickly identify the distribution features of the music group. The semantic detail view displays the music note sequences and combines the window moving to avoid visual clutter while ensuring the presentation of complete semantic details. To prove the usefulness and effectiveness of MUSE, we perform two case studies based on real-world music MIDI data. In addition, we conduct a quantitative user study and an expert evaluation.32/papers/bib/chang2022muse.bib322022-06-01 23:00:00 &#43;0000 UTC32音乐语义模式的可视分析32false3232True32/images/paperimage/chang2022muse.jpg32False32false32IEEE Transactions on Visualization and Computer Graphics322022-06-01 23:00:00 &#43;0000 UTC3290032/papers/pdf/chang2022muse.pdf322022-06-01 23:00:00 &#43;0000 UTC32Chang B, Sun G, Li T, et al. MUSE: Visual Analysis of Musical Semantic Sequence[J]. IEEE Transactions on Visualization and Computer Graphics, 2022.32MUSE: Visual Analysis of Musical Semantic Sequence32publication32/publication/chang2022muse/32/papers/video/chang2022muse.mp4">
                <div class="uk-card uk-card-default" style="position: relative;">
                    <div class="fixy">
                        <div class="uk-card-media-top">
                            
                            <video src="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com//papers/video/chang2022muse.mp4"
                                   controls="" playsinline="" autoplay=false style="width: 100%"></video>
                            
                        </div>
                        <div class="uk-card-body">
                            <h3 class="uk-card-title">音乐语义模式的可视分析</h3>
                        </div>
                    </div>

                    <div class="uk-card-footer">
                        <a href="/publication/chang2022muse/" class="uk-button uk-button-text"
                        >Read more</a
                        >
                    </div>
                </div>
            </li>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            <li data-tags="We introduce a visual analytics method to analyze eye-tracking data and saliency models for dynamic stimuli, such as video or animated graphics. The focus lies on the analysis of the different performance of saliency models in contrast to human observers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with a strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes as well as the attention maps from saliency models can be analyzed in a static three-dimensional representation. We propose algorithms to keep the appearance of the computer’s attention data in line with the human’s eye-tracking data. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data and saliency map. By comparing attention data from both human and computer incorporated with the spatiotemporal characteristics, we are able to find the different patterns within human and computer algorithms. We list our key findings to help developing better saliency detection algorithms.32/papers/bib/liang2016looking.bib322016-12-01 23:00:00 &#43;0000 UTC32时空可视化研究显着性模型32false3232False32/images/paperimage/liang2016looking.jpg32False32false32IEEE Transactions on Multimedia322016-12-01 23:00:00 &#43;0000 UTC3253032/papers/pdf/liang2016looking.pdf322016-12-01 23:00:00 &#43;0000 UTC32Liang H, Liang R, Sun G. Looking into saliency model via space-time visualization[J]. IEEE Transactions on Multimedia, 2016, 18(11): 2271-2281.32Looking Into Saliency Model via Space-Time Visualization32publication32/publication/liang2016looking/32">
                <div class="uk-card uk-card-default" style="position: relative;">
                    <div class="fixy">
                        <div class="uk-card-media-top">
                            
                            <img src="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com//images/paperimage/liang2016looking.jpg">
                            
                        </div>
                        <div class="uk-card-body">
                            <h3 class="uk-card-title">时空可视化研究显着性模型</h3>
                        </div>
                    </div>

                    <div class="uk-card-footer">
                        <a href="/publication/liang2016looking/" class="uk-button uk-button-text"
                        >Read more</a
                        >
                    </div>
                </div>
            </li>
            
            
            
            
            
            <li data-tags="Dimensionality reduction is commonly used for identifying and analyzing patterns in the visual analysis of multi-dimensional datasets. The selection of subspaces is a core building block in projecting high-dimensional data to low-dimensional space, which is usually illustrated as a scatterplot for analysts to easily understand and explore. This process involves human prior knowledge and domain-specific requirements. Thus, quantifying and tracking the changes of projections across subspaces remain challenging. Existing methods can neither quantify the subsets-based changes of dimensionality reduction results when switching subspaces nor automatically and comprehensively display overall and subtle differences among dimensionality reduction results.To address this, we developed EvoSets, a novel visual analytics system designed to help users understand how subspaces affect dimensionality reduction results. The effects are quantified based on subsets&#39; distribution to tracking the sensitivity of dimensionality reduction results across subspaces. System supports the exploration of the overall evolution of the dimensionality reduction results for helping users track the convergence and divergence behavior changes of subsets based on an extended Bubble Sets visualization. Similarities are intuitively illustrated, and dissimilarities are highlighted among projections based on layout constraints. The usefulness and effectiveness are evaluated with a user study and two case studies on multi-dimensional datasets.32/papers/bib/sun2021evosets.bib322021-05-19 23:00:00 &#43;0000 UTC32维度子空间演化的可视分析32false3232True32/images/paperimage/evosets.jpeg32False32false32IEEE Transactions on Big Data322021-05-19 23:00:00 &#43;0000 UTC3250032/papers/pdf/sun2021evosets.pdf322021-05-19 23:00:00 &#43;0000 UTC32Sun G, Zhu S, Jiang Q, et al. EvoSets: TRacking the sensitivity of dimensionality reduction results across subspaces[J]. IEEE Transactions on Big Data, 2021.32EvoSets: Tracking the Sensitivity of Dimensionality Reduction Results Across Subspaces32publication32/publication/sun2021evosets/32/papers/video/sun2021evosets.mp4">
                <div class="uk-card uk-card-default" style="position: relative;">
                    <div class="fixy">
                        <div class="uk-card-media-top">
                            
                            <video src="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com//papers/video/sun2021evosets.mp4"
                                   controls="" playsinline="" autoplay=false style="width: 100%"></video>
                            
                        </div>
                        <div class="uk-card-body">
                            <h3 class="uk-card-title">维度子空间演化的可视分析</h3>
                        </div>
                    </div>

                    <div class="uk-card-footer">
                        <a href="/publication/sun2021evosets/" class="uk-button uk-button-text"
                        >Read more</a
                        >
                    </div>
                </div>
            </li>
            
            
            
            
            
            
            
            
            
            <li data-tags="With the rapid development of mobile Internet, the popularity of video capture devices has brought a surge in multimedia video resources. Utilizing machine learning methods combined with well-designed features, we could automatically obtain video summarization to relax video resource consumption and retrieval issues. However, there always exists a gap between the summarization obtained by the model and the ones annotated by users. How to help users understand the difference, provide insights in improving the model, and enhance the trust in the model remains challenging in the current study. To address these challenges, we propose VSumVis under a user-centered design methodology, a visual analysis system with multi-feature examination and multi-level exploration, which could help users explore and analyze video content, as well as the intrinsic relationship that existed in our video summarization model. The system contains multiple coordinated views, i.e., video view, projection view, detail view, and sequential frames view. A multi-level analysis process to integrate video events and frames are presented with clusters and nodes visualization in our system. Temporal patterns concerning the difference between the manual annotation score and the saliency score produced by our model are further investigated and distinguished with sequential frames view. Moreover, we propose a set of rich user interactions that enable an in-depth, multi-faceted analysis of the features in our video summarization model. We conduct case studies and interviews with domain experts to provide anecdotal evidence about the effectiveness of our approach. Quantitative feedback from a user study confirms the usefulness of our visual system for exploring the video summarization model.32/papers/bib/sun2021VSumVis.bib322021-05-20 23:00:00 &#43;0000 UTC32视频摘要模型的可视理解32false3232True32/images/paperimage/sun2021VSumVis.png32False32false32ACM Transactions on Intelligent Systems and Technology322021-05-20 23:00:00 &#43;0000 UTC3240032/papers/pdf/sun2021VSumVis.pdf322021-05-20 23:00:00 &#43;0000 UTC32Sun G, Wu H, Zhu L, et al. VSumVis: interactive visual understanding and diagnosis of video summarization model[J]. ACM Transactions on Intelligent Systems and Technology (TIST), 2021, 12(4): 1-28.32VSumVis: Interactive Visual Understanding and Diagnosis of Video Summarization Model32publication32/publication/sun2021vsumvis/32/papers/video/VSumVis.mp4">
                <div class="uk-card uk-card-default" style="position: relative;">
                    <div class="fixy">
                        <div class="uk-card-media-top">
                            
                            <video src="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com//papers/video/VSumVis.mp4"
                                   controls="" playsinline="" autoplay=false style="width: 100%"></video>
                            
                        </div>
                        <div class="uk-card-body">
                            <h3 class="uk-card-title">视频摘要模型的可视理解</h3>
                        </div>
                    </div>

                    <div class="uk-card-footer">
                        <a href="/publication/sun2021vsumvis/" class="uk-button uk-button-text"
                        >Read more</a
                        >
                    </div>
                </div>
            </li>
            
            
            
            
            
            <li data-tags="Brain fiber tracts are widely used in studying brain diseases, which may lead to a better understanding of how disease affects the brain. The segmentation of brain fiber tracts assumed enormous importance in disease analysis. In this paper, we propose a novel vector field streamline clustering framework for brain fiber tract segmentations. Brain fiber tracts are firstly expressed in a vector field and compressed using the streamline simplification algorithm. After streamline normalization and regular-polyhedron projection, high-dimensional features of each fiber tract are computed and fed to the IDEC clustering algorithm. We also provide qualitative and quantitative evaluations of the IDEC clustering method and QB clustering method. Our clustering results of the brain fiber tracts help researchers gain perception of the brain structure. This work has the potential to automatically create a robust fiber bundle template that can effectively segment brain fiber tracts while enabling consistent anatomical tract identification.32/papers/bib/xu2021vector.bib322021-07-05 23:00:00 &#43;0000 UTC32脑神经纤维三维空间结构特性的聚类分析32false3232False32/images/paperimage/xu2021vector.png32False32false32IEEE Transactions on Cognitive and Developmental Systems322021-07-05 23:00:00 &#43;0000 UTC3238032/papers/pdf/xu2021vector.pdf322021-07-05 23:00:00 &#43;0000 UTC32Xu C, Sun G, Liang R, et al. Vector field streamline clustering framework for brain fiber tract segmentation[J]. IEEE Transactions on Cognitive and Developmental Systems, 2021.32Vector Field Streamline Clustering Framework for Brain Fiber Tract Segmentation32publication32/publication/xu2021vector/32">
                <div class="uk-card uk-card-default" style="position: relative;">
                    <div class="fixy">
                        <div class="uk-card-media-top">
                            
                            <img src="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com//images/paperimage/xu2021vector.png">
                            
                        </div>
                        <div class="uk-card-body">
                            <h3 class="uk-card-title">脑神经纤维三维空间结构特性的聚类分析</h3>
                        </div>
                    </div>

                    <div class="uk-card-footer">
                        <a href="/publication/xu2021vector/" class="uk-button uk-button-text"
                        >Read more</a
                        >
                    </div>
                </div>
            </li>
            
            
            
            
            
            
            
            
            
            <li data-tags="With the rapid development of society, video surveillance has progressively expanded into different areas of life, such as transportation, security inspection, banks. There are a large number of replaced and newly deployed cameras in fields such as safe cities, smart campuses and smart buildings, which leads to a huge amount of video data, slow retrieval speed in video examining, and low efficiency in understanding complete picture of videos. In this paper, we propose SurVizor, a visual analysis system to understand the key content of surveillance videos. We integrate multiple image features and employ time series analysis methods to explore key temporal patterns in the feature. We integrate multiple visualization views from three levels of video, feature, and frame to promote exploration, analysis and understanding of video content. We evaluate the proposed system through a case study based on real-world surveillance videos from multi-camera and a user study. The results demonstrate the usability and effectiveness of our system in analyzing and understanding the key content of surveillance videos.32/papers/bib/SurVizor.bib322021-11-09 23:00:00 &#43;0000 UTC32视频关键内容的可视分析32false3232True32/images/paperimage/sun2021SurVizor.jpg32False32false32Journal of Visualization322021-11-09 23:00:00 &#43;0000 UTC3230032/papers/pdf/SurVizor.pdf322021-11-09 23:00:00 &#43;0000 UTC32Sun G, Li T, Liang R. SurVizor: visualizing and understanding the key content of surveillance videos[J]. Journal of Visualization, 2022, 25(3): 635-651.32SurVizor: Visualizing and Understanding the Key Content of Surveillance Videos32publication32/publication/sun2021survizor/32/papers/video/SurVizor.mp4">
                <div class="uk-card uk-card-default" style="position: relative;">
                    <div class="fixy">
                        <div class="uk-card-media-top">
                            
                            <video src="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com//papers/video/SurVizor.mp4"
                                   controls="" playsinline="" autoplay=false style="width: 100%"></video>
                            
                        </div>
                        <div class="uk-card-body">
                            <h3 class="uk-card-title">视频关键内容的可视分析</h3>
                        </div>
                    </div>

                    <div class="uk-card-footer">
                        <a href="/publication/sun2021survizor/" class="uk-button uk-button-text"
                        >Read more</a
                        >
                    </div>
                </div>
            </li>
            
            
            
            
            
            
            
            
            
            <li data-tags="Permutation exists in various domains such as mathematics, combinatorics, and computer science. Enumerating each permutation, as well as the multivariate information among different items, allows us, for example, to observe distribution, similarity, and dissimilarity of all possible permutations and select a satisfactory permutation or solution. However, the number of permutations increases dramatically along with the number of items in the permutation, which makes it challenging for users to evaluate potential solutions and identify interesting insights. In this paper, we propose PermVizor, a novel and scalable visualization system that aims assisting users exploring the arrangement, distribution, and comparison of permutations. Necessary and comprehensive analysis of requirements is presented for visualization of permutations. PermVizor enables users to explore overall distribution of each permutation with a glyph-based MDS view, investigate statistical information of selected permutations with a parallel coordinates view, and examine detailed arrangement of the items as well the multivariate information among them for each permutation with pixel-based and block-based PermView. Case studies are conducted on classical datasets such as the axis reordering issue in parallel coordinate data and permutation of traveling salesman problem, which shows that PermVizor could facilitate users in exploring unexpected and desired permutations and confirm their finding and decisions in expected permutations.32/papers/bib/permvizor.bib322019-06-12 23:00:00 &#43;0000 UTC32全排列数据的可视分析32false3232True32/images/paperimage/permvizor.png32False32false32Journal of Visualization322019-06-12 23:00:00 &#43;0000 UTC3230032/papers/pdf/PermVizor.pdf322019-06-12 23:00:00 &#43;0000 UTC32Sun G, Zhou Z, Chang B, et al. PermVizor: visual analysis of multivariate permutations[J]. Journal of Visualization, 2019, 22(6): 1225-1240.32PermVizor: Visual Analysis of Multivariate Permutations32publication32/publication/permvizor/32/papers/video/PermVizor.mp4">
                <div class="uk-card uk-card-default" style="position: relative;">
                    <div class="fixy">
                        <div class="uk-card-media-top">
                            
                            <video src="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com//papers/video/PermVizor.mp4"
                                   controls="" playsinline="" autoplay=false style="width: 100%"></video>
                            
                        </div>
                        <div class="uk-card-body">
                            <h3 class="uk-card-title">全排列数据的可视分析</h3>
                        </div>
                    </div>

                    <div class="uk-card-footer">
                        <a href="/publication/permvizor/" class="uk-button uk-button-text"
                        >Read more</a
                        >
                    </div>
                </div>
            </li>
            
            
            
            
            
            
            
            
            
            
            
            
            
            <li data-tags="Acoustic quality detection is vital in the manufactured products quality control field since it represents the conditions of machines or products. Recent work employed machine learning models in manufac- tured audio data to detect anomalous patterns. A major challenge is how to select applicable audio features to meliorate model’s accuracy and precision. To relax this challenge, we extract and analyze three audio feature types including Time Domain Feature, Frequency Domain Feature, and Cepstrum Feature to help identify the potential linear and non-linear relationships. In addition, we design a visual analysis system, namely AFExplorer, to assist data scientists in extracting audio features and selecting potential feature combinations. AFExplorer integrates four main views to present detailed distribution and relevance of the audio features, which helps users observe the impact of features visually in the feature selection. We perform the case study with AFExplore according to the ToyADMOS and MIMII Dataset to demonstrate the usability and effectiveness of the proposed system.32/papers/bib/wang2022afexplorer.bib322022-04-17 23:00:00 &#43;0000 UTC32音频特征数据的可视交互选择32false3232True32/images/paperimage/wang2022afexplorer.jpg32False32false32Visual Informatics322022-04-17 23:00:00 &#43;0000 UTC3220032/papers/pdf/wang2022afexplorer.pdf322022-04-17 23:00:00 &#43;0000 UTC32Wang L, Sun G, Wang Y, et al. AFExplorer: Visual analysis and interactive selection of audio features[J]. Visual Informatics, 2022, 6(1): 47-55.32AFExplorer: Visual analysis and interactive selection of audio features32publication32/publication/wang2022afexplorer/32/papers/video/wang2022afexplorer.mp4">
                <div class="uk-card uk-card-default" style="position: relative;">
                    <div class="fixy">
                        <div class="uk-card-media-top">
                            
                            <video src="https://zjutvis-1253917436.cos-website.ap-shanghai.myqcloud.com//papers/video/wang2022afexplorer.mp4"
                                   controls="" playsinline="" autoplay=false style="width: 100%"></video>
                            
                        </div>
                        <div class="uk-card-body">
                            <h3 class="uk-card-title">音频特征数据的可视交互选择</h3>
                        </div>
                    </div>

                    <div class="uk-card-footer">
                        <a href="/publication/wang2022afexplorer/" class="uk-button uk-button-text"
                        >Read more</a
                        >
                    </div>
                </div>
            </li>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
        </ul>

        <h2>Reference</h2>
        <div class="uk-text-center">
            
            
            
            
            
            
            
            
            <p style="text-align: -webkit-auto;">[1]Chang B, Sun G, Li T, et al. MUSE: Visual Analysis of Musical Semantic Sequence[J]. IEEE Transactions on Visualization and Computer Graphics, 2022.</p>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            <p style="text-align: -webkit-auto;">[2]Liang H, Liang R, Sun G. Looking into saliency model via space-time visualization[J]. IEEE Transactions on Multimedia, 2016, 18(11): 2271-2281.</p>
            
            
            
            
            
            
            <p style="text-align: -webkit-auto;">[3]Sun G, Zhu S, Jiang Q, et al. EvoSets: TRacking the sensitivity of dimensionality reduction results across subspaces[J]. IEEE Transactions on Big Data, 2021.</p>
            
            
            
            
            
            
            
            
            
            
            <p style="text-align: -webkit-auto;">[4]Sun G, Wu H, Zhu L, et al. VSumVis: interactive visual understanding and diagnosis of video summarization model[J]. ACM Transactions on Intelligent Systems and Technology (TIST), 2021, 12(4): 1-28.</p>
            
            
            
            
            
            
            <p style="text-align: -webkit-auto;">[5]Xu C, Sun G, Liang R, et al. Vector field streamline clustering framework for brain fiber tract segmentation[J]. IEEE Transactions on Cognitive and Developmental Systems, 2021.</p>
            
            
            
            
            
            
            
            
            
            
            <p style="text-align: -webkit-auto;">[6]Sun G, Li T, Liang R. SurVizor: visualizing and understanding the key content of surveillance videos[J]. Journal of Visualization, 2022, 25(3): 635-651.</p>
            
            
            
            
            
            
            
            
            
            
            <p style="text-align: -webkit-auto;">[7]Sun G, Zhou Z, Chang B, et al. PermVizor: visual analysis of multivariate permutations[J]. Journal of Visualization, 2019, 22(6): 1225-1240.</p>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            <p style="text-align: -webkit-auto;">[8]Wang L, Sun G, Wang Y, et al. AFExplorer: Visual analysis and interactive selection of audio features[J]. Visual Informatics, 2022, 6(1): 47-55.</p>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
        </div>
    </div>
    
</section>

<style>
    .paper-abstract{
        margin-left: 4%;
        margin-right: 4%;
        font-size: 20px;
        text-align: justify;
    }
    .refer-pic {
        position: relative;
        width: 50px;
        height: 50px;
    }
    .up-div {
        margin-bottom: 10%;
        width: 100%;
    }
    .big-caption {
        font-size: 32px;
        font-weight: 600;
    }
    @media screen and (max-width: 650px){
        .intro-div {
            display: flex;
            flex-direction: column;
            margin-top: 8%;
        }
        .left-div {
            width: 100%;
            margin-left: 5%;
            text-align: -webkit-center;
        }
        .right-div {
            width: 100%;
            display: flex;
            flex-direction: column;
        }
        .paper-container {
            display: flex;
            flex-direction: column;
            margin-bottom: 4%;
            box-shadow: 0 0 20px rgb(0 0 128 / 30%);
            padding-bottom: 1%;
            width: 100%;
        }
        .small-pic-div {
            display: flex;
            flex-direction: row;
            align-items: center;
            width: fit-content;
        }
    }
    @media screen and (min-width: 650px){
        .intro-div {
            display: flex;
            flex-direction: row;
            margin-top: 8%;
            width: 100%;
        }
        .left-div {
            width: 36%;
            margin-left: 2%;
            margin-right: 1%;
        }
        .right-div {
            width: 60%;
            margin-left: 1%;
            display: flex;
            flex-direction: column;
        }
        .paper-container {
            display: flex;
            flex-direction: row;
            margin-bottom: 4%;
            box-shadow: 0 0 20px rgb(0 0 128 / 30%);
            padding-bottom: 10px;
        }
        .small-pic-div {
            display: flex;
            flex-direction: row;
            align-items: center;
        }
    }
    .round-graph {
        width: 250px;
        height: 250px;
        border-radius: 50%;
        align-items: center;
        justify-content: center;
        overflow: hidden;
        box-shadow: inset 0rem 0rem 1em rgb(0 0 0 / 90%);
    }
    .url-link {
        text-decoration: none;
        margin-right: 2px;
    }
    .small-pic {
        width: 48px;
        height: 48px;
        overflow: hidden;
        transform: scale(1);
        transition: transform 0.1s;
    }
    .intro-content {
        font-size: 24px;
        font-weight: 400;
    }
    .right-down-div {
        display: flex;
        flex-direction: row;
    }
    .interests {
        width: 35%;
    }
    .education {
        width: 65%;
        display: flex;
        flex-direction: column;
    }
    .int-tag {
        display: flex;
        flex-direction: column;
    }
    .interest-content {
        width: max-content;
        margin-bottom: 10px;
        font-weight: 400;
        font-size: 20px;
    }
    .edu-div {
        display: flex;
        flex-direction: column;
    }
    .edu-item {
        display: flex;
        flex-direction: column;
    }
    .edu-content {
        font-size: 20px;
        font-weight: 400;
    }
    .mid-div {
        margin-bottom: 10%;
    }
    .paper-div {
    }
    .list-item {
        list-style-type: none;
    }
    .paper-picture {
        width: 400px;
        margin-right: 15px;
    }
    .paper-content {
        display: flex;
        flex-direction: column;
        width: -webkit-fill-available;
    }
    .paper-title {
        text-align: left;
        font-size: 28px;
        font-weight: 500;
        color: rgb(83,83,83);
    }
    .paper-authors {
        display: flex;
        flex-direction: row;
    }
    .paper-venue{
    }
    .paper-reference {
        display: flex;
        flex-direction: row;
    }
    .paper-author {
        color: #4b4f56;
        font-size: 20px;
        font-weight: 400;
        margin-right: 10px;
    }
    .refer-pic {
        position: relative;
        width: 50px;
        height: 50px;
    }
    .down-div {
        margin-bottom: 10%;
    }
    .contact-div {
        display: flex;
        flex-direction: column;
    }
    .contact-item {
        display: flex;
        flex-direction: row;
        margin-bottom: 5%;
    }
    .contact-pic {
        width: 48px;
        height: 48px;
        margin-right: 10%;
    }
    .contact-content {
        font-size: 24px;
        font-weight: 400;
    }
    .academic-div{
        width: 100%;
        margin-left: 5%;
    }
    .academic-content{
        font-size: 1.5rem;
        font-weight: 400;
    }
</style>


    </div>
<footer class="footer">
    <div class="uk-container">


        <div class="uk-grid-small uk-child-width-1-4@m uk-text-center uk-flex-center uk-margin-medium-bottom" uk-grid>
            <div class="uk-margin-small-bottom">
                <h5 class="uk-margin-small-bottom">邮件</h5>
                <a href="mailto:guodao@zjut.edu.cn">guodao [at] zjut.edu.cn</a>

            </div>
            <div>
                <h5 class="uk-margin-small-bottom">地址</h5>
                <div uk-lightbox>
                    <a href="https://map.baidu.com/search/%E6%B5%99%E6%B1%9F%E7%9C%81%E6%9D%AD%E5%B7%9E%E5%B8%82%E8%A5%BF%E6%B9%96%E5%8C%BA%E7%95%99%E5%92%8C%E8%B7%AF288%E5%8F%B7%E6%B5%99%E6%B1%9F%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E5%B1%8F%E5%B3%B0%E6%A0%A1%E5%8C%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E6%A5%BCc516/" data-caption="浙江省杭州市西湖区留和路288号浙江工业大学屏峰校区计算机大楼C516"
                       data-type="iframe">浙江省杭州市西湖区留和路288号浙江工业大学屏峰校区计算机大楼C516</a>
                </div>
            </div>

        </div>


        <div class="footer-copyright uk-text-center" style="padding-bottom: 1rem">
            <img loading="lazy" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg" style="height: 1.2em;
  width: 1.2em;" alt="Creative Commons">
            <img loading="lazy" src="https://mirrors.creativecommons.org/presskit/icons/by.svg" style="height: 1.2em;
  width: 1.2em;" alt="CC-BY">
            2022
            ZJUT VIS


        </div>
    </div>
</footer>

<!-- JS Plugins -->

<script src="https://zjutvis-1253917436.cos.ap-shanghai.myqcloud.com/cdn/uikit/uikit.min.js"></script>

<script src="https://zjutvis-1253917436.cos.ap-shanghai.myqcloud.com/cdn/uikit/uikit-icons.min.js"></script>



<script type="text/javascript">
var sc_project=12572472;
var sc_invisible=1;
var sc_security="5046b971";

</script>
<script type="text/javascript"
        src="https://www.statcounter.com/counter/counter.js"
        async></script>
<noscript>
  <div class="statcounter"><a title="Web Analytics"
                              href="https://statcounter.com/" target="_blank"><img
          class="statcounter"
          src="https://c.statcounter.com/12572472/0/5046b971/1/"
          alt="Web Analytics"></a></div>
</noscript>
</body>
</html>
